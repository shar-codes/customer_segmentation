{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-10T10:35:16.334724Z","iopub.execute_input":"2021-11-10T10:35:16.335382Z","iopub.status.idle":"2021-11-10T10:35:16.351368Z","shell.execute_reply.started":"2021-11-10T10:35:16.335342Z","shell.execute_reply":"2021-11-10T10:35:16.350400Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In this treatise we'll start out with dataset of customer demographics and corresponding transaction data. The objective will be to train a model to existing customer data and leverage it to predict potential high-value customers for the client. \nThe data has been pre-processed and cleaned accounting for white-spaces, duplicates etc \nWe'll start by dealing with nulls, using appropriate imputation methods. Simple imputation should prove satisfcatory and regression imputation methods needn't be employed. \nTarget encoding or One Hot Encoding can be leveraged to deal with categorical variables as the cardinality is low.\nWe'll then begin building the model. We'll explore various methods starting with supervised methods like decision forest regressors and XGBoost mechanisms, use mutual information to gauge which predictors are useful, and end with unsupervised methods like K-Means clustering and note which method is most effective in minimising the loss function. ","metadata":{}},{"cell_type":"markdown","source":"We begin by splitting the data into training and validation datasets. I'll process the categorical and numerical columns seperately so we'll create lists with categorical and numerical variables. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/kmpg-cleaned-final/KPMG Cleaned New  - CustomerDemographic.csv')\n#dropping rows where RFMScore is null \ndata1 = data.dropna(axis=0,subset=['RFM_Score'])\ny=data1.RFM_Score \nfeatures = ['Gender', 'past_3_years_bike_related_purchases','Age','job_industry_category', 'wealth_segment','owns_car','tenure', 'postcode', 'State', 'Property_Val']\nX= data1[features].copy()\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size= 0.8, test_size = 0.2, random_state=0)\n\ncateg_cols = [cname for cname in X_train.columns if X_train[cname].dtype=='object' ]\nnum_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64','float64'] ]\nprint(X_train.isnull().sum())\nX_train.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.352990Z","iopub.execute_input":"2021-11-10T10:35:16.353602Z","iopub.status.idle":"2021-11-10T10:35:16.448882Z","shell.execute_reply.started":"2021-11-10T10:35:16.353563Z","shell.execute_reply":"2021-11-10T10:35:16.447854Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data =[[10,'Tom'],[15,'Julie'],['NaN','Rob'],[20,'Rov'],[12,'NaN'],[13,'Sanj'],[23,'NaN']]\ndf=pd.DataFrame(data,columns=['Age','Name'])\ndf\n\nimport numpy as np\n\ndf['Name'] = df['Name'].replace('NaN', np.nan)\ndf['Age'] = df['Age'].replace('NaN', np.nan)\ndf1 = df.dropna(axis=0, subset=['Name'])\ndf1","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.452810Z","iopub.execute_input":"2021-11-10T10:35:16.453257Z","iopub.status.idle":"2021-11-10T10:35:16.478150Z","shell.execute_reply.started":"2021-11-10T10:35:16.453224Z","shell.execute_reply":"2021-11-10T10:35:16.477208Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Dealing with nulls in numerical data using Simple Imputer...","metadata":{}},{"cell_type":"code","source":"#Taking care of numerical nulls with Simple Imputer \nfrom sklearn.impute import SimpleImputer\n\nX_num_t = X_train[num_cols]\nX_num_v = X_valid[num_cols]\n\nmy_imputer = SimpleImputer(strategy= 'median')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_num_t))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_num_v))\n\nimputed_X_train.columns = X_num_t.columns\nimputed_X_valid.columns = X_num_v.columns\n\nimputed_X_train.head()\nprint(imputed_X_train.isnull().sum())\nimputed_X_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.479786Z","iopub.execute_input":"2021-11-10T10:35:16.480070Z","iopub.status.idle":"2021-11-10T10:35:16.509651Z","shell.execute_reply.started":"2021-11-10T10:35:16.480041Z","shell.execute_reply":"2021-11-10T10:35:16.508696Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Dealing with nulls in categorical data using the most frequent observation","metadata":{}},{"cell_type":"code","source":"#Imputing most frequent observation in Nulls \nfrom sklearn.impute import SimpleImputer\ncateg_imputer = SimpleImputer(strategy='most_frequent')\nX_cat_t = X_train[categ_cols]\nX_cat_v = X_valid[categ_cols]\n\nX_imp_cat_train = pd.DataFrame(categ_imputer.fit_transform(X_cat_t))\nX_imp_cat_val = pd.DataFrame(categ_imputer.transform(X_cat_v))\n\nX_imp_cat_train.columns = X_cat_t.columns\nX_imp_cat_val.columns= X_cat_v.columns\n\nX_imp_cat_train.head()\nprint(X_imp_cat_train.isnull().sum())\n\nX_imp_cat_train.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.510743Z","iopub.execute_input":"2021-11-10T10:35:16.511007Z","iopub.status.idle":"2021-11-10T10:35:16.554240Z","shell.execute_reply.started":"2021-11-10T10:35:16.510982Z","shell.execute_reply":"2021-11-10T10:35:16.553258Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Concatting imputed categorical and numerical data\nX_train_new = pd.concat([imputed_X_train, X_imp_cat_train], axis=1)\nX_valid_new = pd.concat([imputed_X_valid, X_imp_cat_val], axis=1)\nX_train_new.head()\n\nX_train_new.shape[0]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.555847Z","iopub.execute_input":"2021-11-10T10:35:16.556537Z","iopub.status.idle":"2021-11-10T10:35:16.567235Z","shell.execute_reply.started":"2021-11-10T10:35:16.556490Z","shell.execute_reply":"2021-11-10T10:35:16.566173Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Models arent trained to accept categorical varaibles in raw form so we'll numerically encode such variables. Since the cardinality of varaibles in this dataset is low, we'll employ One Hot Encoding.","metadata":{}},{"cell_type":"code","source":"#OneHotEncoding for categorical variables\n\ns = (X_train_new.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nfrom sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse = False)\nOH_X_train= pd.DataFrame(OH_encoder.fit_transform(X_train_new[object_cols]))\nOH_X_valid= pd.DataFrame(OH_encoder.transform(X_valid_new[object_cols]))\nOH_X_train.head()\n\n#Putting index back \nOH_encoder.get_feature_names()\nOH_X_train.columns= OH_encoder.get_feature_names()\nOH_X_valid.columns =  OH_encoder.get_feature_names()\nOH_X_valid.head()\n\n#remove categ columns we'll replace with OHE cols \nnum_X_train_new = X_train_new.drop(categ_cols, axis=1)\nnum_X_valid_new = X_valid_new.drop(categ_cols, axis=1)\n\n#final data with OHE columns \nfin_X_train = pd.concat([num_X_train_new,OH_X_train], axis=1)\nfin_X_valid = pd.concat([num_X_valid_new,OH_X_valid], axis=1)\n\nfin_X_train.columns\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.568389Z","iopub.execute_input":"2021-11-10T10:35:16.568748Z","iopub.status.idle":"2021-11-10T10:35:16.604264Z","shell.execute_reply.started":"2021-11-10T10:35:16.568714Z","shell.execute_reply":"2021-11-10T10:35:16.603166Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We'll start with a basic Random Forest Regressor model and tune parameters to reduce the chosen loss function- here - the mean absolute error","metadata":{}},{"cell_type":"code","source":"#Lets build a model!\n#method1 using RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = RandomForestRegressor(n_estimators=200,random_state =0)\nmodel.fit(fin_X_train,y_train)\npreds= model.predict(fin_X_valid)\nmae = mean_absolute_error(y_valid,preds)\nmae","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:16.605534Z","iopub.execute_input":"2021-11-10T10:35:16.605830Z","iopub.status.idle":"2021-11-10T10:35:20.217461Z","shell.execute_reply.started":"2021-11-10T10:35:16.605801Z","shell.execute_reply":"2021-11-10T10:35:20.216403Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"This value is a little high. We'll explore other models to try to minimise the mae. New we explore a XGBRegressor model","metadata":{}},{"cell_type":"code","source":"#Using XGBoost model \nfrom xgboost import XGBRegressor\n\nmodel_xgb = XGBRegressor(n_estimators=500, learning_rate=0.95)\nmodel_xgb.fit(fin_X_train,y_train,\n             early_stopping_rounds=5,\n             eval_set=[(fin_X_valid,y_valid)],\n             verbose= False)\npredictions = model_xgb.predict(fin_X_valid)\nmae2 = mean_absolute_error(predictions,y_valid)\nmae2\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:20.221118Z","iopub.execute_input":"2021-11-10T10:35:20.221427Z","iopub.status.idle":"2021-11-10T10:35:20.279340Z","shell.execute_reply.started":"2021-11-10T10:35:20.221400Z","shell.execute_reply":"2021-11-10T10:35:20.278422Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Though on the surface the mean absolute error seems to have reduced,let's take a look at whats going on under the hood -","metadata":{}},{"cell_type":"code","source":"print(predictions[5])\nprint(predictions[34])\nprint(predictions[272])","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:38:28.851192Z","iopub.execute_input":"2021-11-10T10:38:28.851802Z","iopub.status.idle":"2021-11-10T10:38:28.857770Z","shell.execute_reply.started":"2021-11-10T10:38:28.851757Z","shell.execute_reply":"2021-11-10T10:38:28.856655Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Each value in the list of predictions seems to be identical, ie, the model seems to be predicting the same RFM score for every customer. Before drawing conclusions we'll ensure each value in the list 'predictions' is indeed the same ","metadata":{}},{"cell_type":"code","source":"x=0\nfor n in predictions :\n    if n==n+1:\n        x=x+1\nprint(x)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:48:31.431060Z","iopub.execute_input":"2021-11-10T10:48:31.431455Z","iopub.status.idle":"2021-11-10T10:48:31.444340Z","shell.execute_reply.started":"2021-11-10T10:48:31.431422Z","shell.execute_reply":"2021-11-10T10:48:31.443515Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"The above code returns 0 which tells us that every element in the list is indeed the same and the model is predicting the same RFM value for each customer. ie the model is grossly overfitted. \nPerhaps the data isn't large enough to identify trends, or perhaps no strong relationships exist between variables. Before proceeding, we'll subject our varaibles to a mutual informaation regression to gauge how useful each parameter is in predicting the RFM score \n","metadata":{}},{"cell_type":"code","source":"#mutual information\nfrom sklearn.feature_selection import mutual_info_regression\nX= X.copy()\n\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y, random_state =0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(fin_X_train, y_train)\nmi_scores\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T10:35:20.283144Z","iopub.execute_input":"2021-11-10T10:35:20.285000Z","iopub.status.idle":"2021-11-10T10:35:21.768111Z","shell.execute_reply.started":"2021-11-10T10:35:20.284951Z","shell.execute_reply":"2021-11-10T10:35:21.767401Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Neither of the mutual information scores seem to be very high so instrinsically none of these variables seem to be very good predictors. For the purpose of experiment we'll attempt to use K-Means clsutering to segment the customer base. We'll then add the cluster identifier to the original dataset and see if that trains a more accurate model","metadata":{}},{"cell_type":"code","source":"#Ok Let's Try K-Means LMAO hahahahhAAAAAAAAAaaaaaaa\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    score = cross_val_score(\n        model, X, y, cv=5, scoring=\"neg_mean_absolute_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\nfin_Xt = fin_X_train.copy()\nfin_Xv = fin_X_valid.copy()\nfin_y = y_train.copy()\n\nscale = StandardScaler()\nfin_X_t_scaled = pd.DataFrame(scale.fit_transform(fin_Xt))\nfin_X_v_scaled = pd.DataFrame(scale.fit_transform(fin_Xv))\n\nfin_X_t_scaled.columns = fin_X_train.columns\nfin_X_v_scaled.columns = fin_X_valid.columns\n\n#We ask the model to identify clusters then add the clusters parameter as a new factor to the final data.\n#We then run the XGBRegressor function again\nkmeans = KMeans(n_clusters=5, n_init=7, random_state=0)\nfin_Xt[\"Cluster\"] = kmeans.fit_predict(fin_X_t_scaled)\nfin_Xv[\"Cluster\"] = kmeans.fit_predict(fin_X_v_scaled)\n\nprint(fin_Xt.head())\n\nmodel_new = XGBRegressor(n_estimators=500, learning_rate=0.95)\nmodel_new.fit(fin_Xt,y_train,\n             early_stopping_rounds=7,\n             eval_set=[(fin_Xv,y_valid)],\n             verbose= False)\npredict = model_new.predict(fin_Xv)\nmae2 = mean_absolute_error(predict,y_valid)\nprint(mae2)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-10T11:01:30.552532Z","iopub.execute_input":"2021-11-10T11:01:30.553528Z","iopub.status.idle":"2021-11-10T11:01:33.949175Z","shell.execute_reply.started":"2021-11-10T11:01:30.553468Z","shell.execute_reply":"2021-11-10T11:01:33.948192Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"In conclusion our model hasn't been altogether extremely successful in identiying patterns and predicting scores for new customers that we could use to predict the potential of a new dataset of consumers. If we were simply looking to segment our existing consumer database K-Means algorithms would be satisfactory, however this may fall short in terms of predicting high-potential consumers. The best way was to add the cluster identifiers to the orginal data and train a supervised model on the same, but here too, the results were unsatisfactory.","metadata":{}}]}